for (i in 1:num_simulations){
winning[i] = once() * bet_size
}
return(mean(winning))
}
winning = craps_game()
table(winning)
prop.table(table(winning))
craps_game(100000, 100)
#投擲兩顆骰子的總和
dice_sum = function(){
return(sum(sample(1:6, replace = TRUE)))
}
#一次投擲
once = function(){
num = 1
while (TRUE){
#第一次投擲
if (num == 1){
first_roll = dice_sum()
if (first_roll %in% c(7, 11)){
return(1)
}else if (first_roll %in% c(2, 3, 12)){
return(-1)
}
#其他次投擲
}else{
other_sum = dice_sum()   #讓骰子和 = other_sum
if (other_sum == first_roll){
return(1)
}else if (other_sum == 7){
return(-1)
}
}
num = num + 1
}
}
craps_game = function(num_simulations = 1000, bet_size = 100){
winning = c()
for (i in 1:num_simulations){
winning[i] = once() * bet_size
}
return(meanwinning)
}
winning = craps_game()
#投擲兩顆骰子的總和
dice_sum = function(){
return(sum(sample(1:6, replace = TRUE)))
}
#一次投擲
once = function(){
num = 1
while (TRUE){
#第一次投擲
if (num == 1){
first_roll = dice_sum()
if (first_roll %in% c(7, 11)){
return(1)
}else if (first_roll %in% c(2, 3, 12)){
return(-1)
}
#其他次投擲
}else{
other_sum = dice_sum()   #讓骰子和 = other_sum
if (other_sum == first_roll){
return(1)
}else if (other_sum == 7){
return(-1)
}
}
num = num + 1
}
}
craps_game = function(num_simulations = 1000, bet_size = 100){
winning = c()
for (i in 1:num_simulations){
winning[i] = once() * bet_size
}
return(winning)
}
winning = craps_game()
table(winning)
prop.table(table(winning))
library(datasets)
library(MASS)
library(tidyverse)
data(iris)
head(iris)
pairs(iris, col=iris$Species)
trainI <- sample(1:150, 75)
traind <- iris[trainI,]
testd <- iris[-trainI,]
iris$index =c(1:nrow(iris))
train_df <- iris %>% group_by(Species) %>% sample_frac(0.8)
test_df  <- anti_join(iris, train_df, by = 'index')
traind <-as.data.frame(train_df[,-6])
testd <-as.data.frame(test_df[,-6])
pred=knn(traind[,1:4], testd[,1:4], cl=traind[,5], k = 6) #prob=T cl為class
table(real=testd[,5], pred)
### KNN ###
library(class)
pred=knn(traind[,1:4], testd[,1:4], cl=traind[,5], k = 6) #prob=T cl為class
table(real=testd[,5], pred)
#choose k
range <- 1:round(0.2 * nrow(traind)) #通常 k 的上限為訓練樣本數的 20%
for (i in range) {
test_predicted <- knn(train = traind[,1:4], test = testd[,1:4], cl = traind[,5], k = i)
conf_mat <- table(testd$Species, test_predicted)
accuracies[i] <- sum(diag(conf_mat))/sum(conf_mat)
}
accuracies <- rep(NA, length(range))
for (i in range) {
test_predicted <- knn(train = traind[,1:4], test = testd[,1:4], cl = traind[,5], k = i)
conf_mat <- table(testd$Species, test_predicted)
accuracies[i] <- sum(diag(conf_mat))/sum(conf_mat)
}
##視覺化上面結果
plot(range, accuracies, xlab = "k")
which.max(accuracies) #k
### Bagging ###
install.packages("ipred")
library(ipred)
bag <- bagging(Species ~ ., data = iris[,-6], coob=TRUE)
print(bag)
pred <- predict(bag, testd)
table(Real = testd$Species, Predict = pred)
### Boosting ###
#install.packages("caret", dependencies = c("Depends", "Suggests"))
install.packages("adabag")
library(adabag)
boo <- boosting(Species ~ ., data = traind, mfinal = 100)
pred <- predict(boo,testd)
pred
pred$confusion
### Boosting ###
#install.packages("caret", dependencies = c("Depends", "Suggests"))
install.packages("adabag")
library(adabag)
boo <- boosting(Species ~ ., data = traind, mfinal = 100)
install.packages("adabag")
install.packages("adabag")
### XGBoost ###
install.packages("xgboost")
library(xgboost)
X_train = data.matrix(traind[,-5])
y_train = as.integer(traind[,5])-1
X_test = data.matrix(testd[,-5])
y_test = as.integer(testd[,5] )-1
# convert the train and test data into xgboost matrix type.
xgboost_train = xgb.DMatrix(data=X_train, label=y_train)
xgboost_test = xgb.DMatrix(data=X_test, label=y_test)
# train a model using our training data
xgb_params <- list(
booster = "gbtree",
eta = 0.01,
max_depth = 8,
gamma = 4,
subsample = 0.75,
colsample_bytree = 1,
objective = "multi:softprob", #multiclassification
eval_metric = "mlogloss",
num_class = 3 #length(levels(traind$Species))
)
xgb_model <- xgb.train(
params = xgb_params,
data = xgboost_train,
nrounds = 5000,
verbose = 1
)
xgb_model
#make predictions on test data
xgb_preds <- predict(xgb_model, as.matrix(X_test), reshape = TRUE)
xgb_preds <- as.data.frame(xgb_preds)
colnames(xgb_preds) <- levels(iris$Species)
xgb_preds
### LightGBM ###
library(lightgbm)
dtrain = lgb.Dataset(X_train, label = y_train)
dtest = lgb.Dataset.create.valid(dtrain, data = X_test, label = y_test)
# define parameters
params = list(
objective= 'multiclass',
metric = "multi_error",
num_class= 3
)
# validataion data
valids = list(test = dtest)
model = lgb.train(params,
dtrain,
nrounds = 100,
valids,
min_data=1,
learning_rate = 1,
early_stopping_rounds = 10)
print(model$best_score)
library(AER)
data("CreditCard")
View(CreditCard)
trains <- sample(1:1319, 660)
trained <- [trains]
trained <- CreditCard[trains]
n <- nrow(CreditCard)
trained <- CreditCard[trains]
trains <- sample(1:n, size = round(0.5*n))
trained <- CreditCard[trains]
pairs(CreditCard, col=CreditCard$card)
n <- nrow(CreditCard)
trains <- sample(1:n, size = round(0.5*n))
trained <- CreditCard[trains]
tested <- CreditCard[-trains]
### Decision tree ###
install.packages("rpart")
install.packages("rpart.plot")
install.packages("rpart")
install.packages("rpart")
### Decision tree ###
install.packages("rpart")
install.packages("rpart")
library(rpart)
trees <- rpart(card ~. ,data=traind, method="class")
data(iris)
View(iris)
trees <- rpart(card ~. ,data=trained, method="class")
trained <- CreditCard[trains]
library(AER)
data("CreditCard")
pairs(CreditCard, col=CreditCard$card)
n <- nrow(CreditCard)
trains <- sample(1:n, size = round(0.5*n))
trained <- CreditCard[trains]
tested <- CreditCard[-trains]
library(rpart)
trees <- rpart(card ~. ,data=trained, method="class")
trained <- CreditCard$trains
tested <- CreditCard[-trains]
trees <- rpart(card ~. ,data=trained, method="class")
trained <- CreditCard[trains]
trained <- CreditCard[trains,]
tested <- CreditCard[-trains,]
trees <- rpart(card ~. ,data=trained, method="class")
preds <- predict(tree, newdata=tested, type="class")
View(iris)
library(rpart)
preds <- predict(tree, newdata=tested, type="class")
preds <- predict(trees, newdata=tested, type="class")
table(Real = testd$card, Predict = pred)
table(Real = testd$card, Predict = preds)
table(Real = tested$card, Predict = preds)
rpart.plot(trees)
install.packages("rpart.plot")
library(rpart.plot)
rpart.plot(trees)
rpart.rules(trees,cover=T)
install.packages("randomForest")
#Random Forest
library(randomForest)
rfs <- randomForest(card ~ ., data = trained, importance = TRUE)
plot(rfs)
pred=predict(rfs, newdata = tested)
table(Real = tested$card, Predict = pred)
rpart.plot(rfs)
rpart.rules(rfs,cover=T)
airline_survey <- read.csv("C:/Users/ingri/R learning/airline_survey.csv")
View(airline_survey)
#1 監督式：預測滿意度
library(tidyverse)
ggplot(airline_survey, mapping = aes(x = "Gender", y = "satisfication"))
male <- airline_survey %>%
filter(Gender == 'Male')
View(male)
female <- airline_survey %>%
filter(Gender == 'Female')
satisfy1 <- data.frame(table(male$satisfaction))
colnames(satisfy1) <- c("Male", "satisfication")
satisfy1$Prec <- satisfy1$satisfaction / sum(satisfy1$satisfication)*100
colnames(satisfy1) <- c("Male", "Freq")
satisfy1$Prec <- satisfy1$Freq / sum(satisfy1$Freq)*100
satisfy2 <- data.frame(table(female$satisfaction))
colnames(satisfy2) <- c("Female", "Freq")
satisfy2$Prec <- satisfy2$Freq / sum(satisfy2$Freq)*100
par(mfrow = c(1, 2))
barplot(satisfy1$Prec,
main = "Male",
names.arg = c('satisfied', 'neutral or dissatisfied')
)
barplot(satisfy2$Prec,
main = "Female",
names.arg = c('satisfied', 'neutral or dissatisfied')
)
satisfy1 <- airline_survey %>%
filter(satisfication == 'satisfied')
satisfy2 <- airline_survey %>%
filter(satisfication == 'neutral or dissatisfied')
#1 監督式：預測滿意度
library(tidyverse)
satisfy1 <- airline_survey %>%
filter(satisfaction == 'satisfied')
airline_survey <- read.csv("C:/Users/ingri/R learning/airline_survey.csv")
View(airline_survey)
View(airline_survey)
satisfy1 <- airline_survey %>%
filter(satisfaction == 'satisfied')
satisfy2 <- airline_survey %>%
filter(satisfaction == 'neutral or dissatisfied')
##性別對滿意度的影響
gender1 <- data.frame(table(satisfy1$Gender))
colnames(gender1) <- c("Gender", "Freq")
gender1$Prec <- gender1$Freq / sum(gender1$Freq)*100
gender2 <- data.frame(table(satisfy2$Gender))
colnames(gender2) <- c("Gender", "Freq")
gender2$Prec <- gender2$Freq / sum(gender2$Freq)*100
par(mfrow = c(1, 2))
barplot(gender1$Prec,
main = "satisfied",
names.arg = c('Male', 'Female')
)
barplot(gender2$Prec,
main = "neutral or dissatisfied",
names.arg = c('Male', 'Female')
)
##customer tyoe對滿意度的影響
type1 <- data.frame(table(satisfy1$Customer.Type))
colnames(type1) <- c("Customer.Type", "Freq")
type1$Prec <- type1$Freq / sum(type1$Freq)*100
type2 <- data.frame(table(satisfy2$Customer.Type))
colnames(type2) <- c("Customer.Type", "Freq")
type2$Prec <- type2$Freq / sum(type2$Freq)*100
barplot(type1$Prec,
main = "satisfied",
names.arg = c('loyal', 'disloyal')
)
barplot(gender2$Prec,
main = "neutral or dissatisfied",
names.arg = c('loyal', 'disloyal')
)
airline_survey$satisfaction <- ifelse(airline_survey$satisfaction == "satisfied", 1, 0)
View(airline_survey)
View(airline_survey)
Female <-
airline_survey %>%
filter(Gender == "Female")
fit1 <- lm(Flight.Distance~satisfaction, data = Female)
summary(fit1)
View(Female)
ggplot(data = Female,
aes(x = Flight.Distance, y = satisfaction)) +
geom_point()+ geom_smooth(method = "lm")
fit1 <- lm(satisfaction~Flight.Distance, data = Female)
summary(fit1)
ggplot(data = Female,
aes(x = Flight.Distance, y = satisfaction)) +
geom_point()+ geom_smooth(method = "lm")
#month是類別變數，所以畫不出來，要轉換成numberical
plot(Female$Flight.Distance, Female$satisfaction,xlab="Distance", ylab="Satisfaction")
curve (cbind(1,x) %*% coef(fit1), add=TRUE,col="blue")
abline(fit1)
fit1 <- lm(satisfaction~., data = Female)
summary(fit1)
library(tidyverse)
iris %>%
ggplot(aes(x=Sepal.Length, y=Sepal.Width ,color=Species,shape=Species)) +
geom_point() #scatterplot
iris %>%
ggplot(aes(x=Sepal.Length)) +
geom_histogram() #histogram
ggplot(iris, aes(x=Sepal.Length, color=Species)) +
geom_histogram(fill="white",bins=10)
ggplot(iris, aes(x=Sepal.Length, color=Species,fill=Species)) +
geom_histogram(bins=10)
##-----------------------------------------------------------
library(cowplot)
ii <- iris %>%
ggplot(aes(x=Sepal.Length, y=Sepal.Width ,color=Species,shape=Species)) +
geom_point(size=2) +
theme_cowplot()
#add background
png(file = "scatter.png", width = 3840, height = 2160, units = "px", res = 72*4) #export the plot
ggdraw() +
draw_image("flower.jpeg", scale = 0.5) + # the background for the plot
draw_plot(ii)
library(corrgram)
corr = cor(iris[,1:4])
corrgram(iris)
library(corrplot)
library(gplots)
heatmap.2(as.matrix(t(iris[,1:4])),dendrogram ="none",trace="none",Rowv = F,Colv = F)
heatmap.2(as.matrix(t(iris[,1:4])),dendrogram ="column",trace="none")
symnum(corr)
heatmap.2(corr, Rowv=FALSE, symm=TRUE, trace="none",
cexRow=0.8, cexCol=0.8,srtCol=45,srtRow =0 )
library(RColorBrewer)
cols = colorRampPalette(rev(brewer.pal(9, "Reds")))(1000)
cols = colorRampPalette(brewer.pal(9, "Reds"))(1000)
heatmap.2(as.matrix(t(iris[,1:4])),dendrogram ="column",trace="none",
cexRow=0.8, cexCol=0.8,srtCol=45,srtRow =0,
col=cols,key=TRUE) #srt用來調角度
library(jpeg)
# Libraries
library(tidyverse)
library(plotly)
# Scatterplot
p = ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width, color=Species, shape=Species)) +
geom_point(size=6, alpha=0.6)
p
ggplotly(p)
plot_ly(iris, x = ~Sepal.Length, y = ~Sepal.Width,  type="scatter", mode = "markers" , color = ~Species ,
colors="Set1",
marker=list( size=20 , opacity=0.5)  )
plot_ly(iris, x = ~Sepal.Length, y = ~Sepal.Width , type="scatter", mode = "markers",
marker=list( size=20 , opacity=0.5), color = ~Sepal.Length ,
colors=c("navyblue","royalblue","skyblue","yellow","pink" ))
# Histogram
plot_ly(iris, x = ~Sepal.Length, type = "histogram")
# separate by species
plot_ly(iris,x = ~Sepal.Length, y = ~Sepal.Width, type = "scatter", mode = "markers", sizes = c(10, 1000), frame = ~Species)
normalizedFun <- function(x){
maxmin <- range(x, na.rm = TRUE)
ans <- (x - maxmin[1]) / (maxmin[2] - maxmin[1])
return(ans)
}
normalizedFun(df$c)
df <- data.frame(
a = rnorm(10), # Generate 10 standard normal random numbers 隨機取10個
b = rnorm(10),
c = rnorm(10),
d = rnorm(10)
)
normalizedFun(df$c)
bmi_cal = function(height, weight){
height = height/100
bmi = weight / height^2
return(bmi)
}
bmi_cal(155,52)
if (sample(1:10,1) > 5) message("Bingo!")
switch(2, #指定執行第二行程式碼，故回傳4
2+3,
2^2,
3*6)
my.lunch <- function(y){
switch(y,
poor="Noodle",
ok="McD",
rich="Starbucks")
}
my.lunch("poor")
for(bloodtype in c("A","B","O","AB")){
cat(bloodtype, "\t") #cat()print出字串的其他方式
}
for(bloodtype in c("A","B","O","AB")){
cat("my blood type is", bloodtype, "\n")
}
##SRS
library(foreign)
library(AER)
setwd("C:/Users/ingri/R learning/Lecture 78-20230401")
data("CreditCard")
CreditCard$card <- ifelse(CreditCard$card == "yes", 1, 0) #you can define "y" first
table(CreditCard$card) #frequency table
##binary → logit； count → log
##### logistic regression ##### binary的分類方法
# fit model
logit_model <- glm(card ~ reports + age + income + owner, data=CreditCard, family=binomial(link="logit"))
summary(logit_model)
temp=predict(logit_model, type = "response") #P(Y=1)
pred=ifelse(temp>0.5,1,0) #usually we use 0.5 as the threshold
table(Predict=pred,True=CreditCard$card) #做出type1, type2  error
# 95% CI for beta
confint(logit_model)
# to exponentiate for odds ratios with CI’s 取消log操作
exp(cbind(OR = coef(logit_model), confint(logit_model)))
# Analysis of Deviance Table (Goodness of Fit)
anova(logit_model, test = "Chi")
# predict confusion matrix混淆矩陣 預測和已知的結果有沒有差異
new <- data.frame(reports = 0, age = 30, income = 10, owner = "yes")
View(new)
result <- predict(logit_model, newdata = new, type = "response")
result #prob(y=1)
odds = exp(coef(logit_model)[1]+30*coef(logit_model)[3]+10*coef(logit_model)[4]+coef(logit_model)[5])
odds/(1+odds) #P(y=1)
odds/(1+odds) #P(y=1)
library(arm) #invlogit(x beta) = P(y=1)
invlogit(coef(logit_model)[1]+30*coef(logit_model)[3]+10*coef(logit_model)[4]+coef(logit_model)[5]) #prob(y=1)
# stepwise
step(logit_model)
library(MASS)
stepAIC(logit_model)
# creat training and test data 把數據切成兩部分，一部份用來training；一部份用來testing
ones <- CreditCard[which(CreditCard$card == "yes"), ]  # all yes's
zeros <- CreditCard[which(CreditCard$card == "no"), ]  # all no's
set.seed(100)  # for repeatability of samples
train1_index <- sample(1:nrow(ones), 0.7*nrow(ones))  # yes's for training
train0_index <- sample(1:nrow(zeros), 0.7*nrow(zeros))  # no's for training. Pick as many no's as yes's
train1 <- ones[train1_index, ]
train0 <- zeros[train0_index, ]
traind <- rbind(train1, train0)  # row bind the yes's and no's
# Create Test Data
test1 <- ones[-train1_index, ]
test0 <- zeros[-train0_index, ]
testd <- rbind(test1, test0)
# training and test by tidyverse
library(tidyverse)
CreditCard$index =c(1:nrow(CreditCard))
View(CreditCard)
train_df <- CreditCard %>% group_by(card) %>% sample_frac(0.7)
test_df  <- anti_join(CreditCard, train_df, by = 'index') #anti_join 分開
View(test_df)
# fit model
model <- glm(card ~ reports + age + income + owner, data=train_df, family=binomial(link="logit"))
summary(model)
predicted1 <- plogis(predict(model, test_df)) #prob(y=1), the same as predicted2
predicted2 <- predict(model, test_df, type="response")  #prob(y=1)
