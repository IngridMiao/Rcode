predicted1 <- plogis(predict(model, test_df)) #prob(y=1), the same as predicted2
predicted2 <- predict(model, test_df, type="response")  #prob(y=1)
# find cut point (threshold)
library(MKclass)
threshold = optCutoff(predicted2, truth = test_df$card, namePos = "yes")
predresult = ifelse(predicted2>threshold[1],"yes","no")
# another way to fit a glm model
library(caret)
model2 <- train(card ~ reports + age + income + owner,
data = train_df,
method = "glm",
family = "binomial")
summary(model2)
# find cut point (threshold)
library(MKclass)
threshold = optCutoff(predicted2, truth = test_df$card, namePos = "yes")
predresult = ifelse(predicted2>threshold[1],"yes","no")
threshold = optCutoff(predicted2, truth = test_df$card, namePos = "yes")
threshold = optCutoff(predicted2, truth = test_df$card, namePos = "yes")
# validation
summary(model)
library(car)
vif(model) #all<4
install.packages("pROC")
install.packages("pROC")
install.packages("pROC")
library(pROC)
install.packages("pROC")
install.packages("pROC")
library(pROC)
temp <- roc(test_df$card, predicted2)
plot(temp)
auc(temp) #area under the curve
library(tidyverse)
roc.data <- tibble(x = 1-temp$specificities, y = temp$sensitivities)
ggplot(roc.data, aes(x = x, y = y)) +
geom_line() +
#scale_x_reverse() +  #if use, no "1-" in x above
ylab("Sensitivity") +
xlab("1-Specificity")
ggplot(roc.data, aes(x = x, y = y)) +
geom_line() +
#scale_x_reverse() +  #if use, no "1-" in x above
ylab("Sensitivity") +
xlab("1-Specificity")
# selection via AUC
model2 <- glm(card ~ age + income, data=train_df, family=binomial(link="logit"))
# selection via AUC
model2 <- glm(card ~ age + income, data=train_df, family=binomial(link="logit"))
predicted_m2 <- predict(model2, test_df, type="response")  # predicted scores
## easy way
temp2 <- roc(test_df$card, predicted_m2)
plot(temp)
plot(temp2, add=TRUE, col='red') #black one is better
temp2$auc #compare auc with the one from model1
## ggplot way
roc1.data <- tibble(x = temp$specificities,
y = temp$sensitivities,
model = "model1")
roc2.data <- tibble(x = temp2$specificities,
y = temp2$sensitivities,
model = "model2")
roc.both <- rbind(roc1.data, roc2.data)
ggplot(roc.both, aes(x=x,y=y,color=model)) +
geom_line() +
scale_x_reverse()
##### Poisson ######
ggplot(CreditCard,aes(x=active)) +
geom_histogram(binwidth=5)
pmodel <- glm(active ~ reports + age + income + owner, data=CreditCard, family=poisson)
summary(pmodel)
predict(pmodel, type="response")
exp(coef(pmodel)) #coef. explanation
install.packages("qcc")
library(qcc)
qcc.overdispersion.test(CreditCard$active, type = "poisson")
library(AER)
dispersiontest(pmodel)
list(res.deviance = deviance(pmodel), df = df.residual(pmodel),
p = pchisq(deviance(pmodel), df.residual(pmodel), lower.tail=FALSE)) #sig. means not a good fit.
#### For rates: offset
data(Fatalities)
# offset model
model1 <- glm(fatal ~ year, offset = log(milestot),family = poisson, data=Fatalities)
model1 <- glm(fatal ~ year + offset(log(milestot)),family = poisson, data=Fatalities)
#7 exercise
##1
library(ISLR)
summary(Smarket)
data(Smarket)
force(Smarket)
library(AER)
Smarket$Direction <- ifelse(Smarket$direction == "up", 1, 0)
Smarket$Direction <- ifelse(Smarket$Direction == "up", 1, 0)
table(Smarket$Direction)
View(Smarket)
data(Smarket)
Smarket$Direction <- ifelse(Smarket$Direction == "Up", 1, 0)
table(Smarket$Direction)
View(Smarket)
logit_model <- glm(Volume ~ Lag1 + Lag2 + Lag3 + Lag4 +Lag5, data = Smarket, family = binomial(link = logit))
logit_model <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 +Lag5, data = Smarket, family = binomial(link = logit))
View(logit_model)
logit_model <- glm(Direction ~ Volume, data = Smarket, family = binomial(link = logit))
View(logit_model)
###2001~2004 trained ; 2005 tested
index = which(Year == 2005)
###2001~2004 trained ; 2005 tested
index = which(Smarket$Year == 2005)
trained = Smarket[-index,]
tested = Smarket[index,]
###
fit <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 +Lag5 + Volume, data = trained, family = binomial(link = logit))
summary(fit)
###
temp = predict(fit, new = tested, type = 'response')
pred = ifelse(temp > 0.5,"Up","Down")
table(Predict = pred, tested$Direction)
### accurary rate
(77+44)/252
Smarket$Direction <- ifelse(Smarket$Direction == 'Up',1,0)
pred = ifelse(temp > 0.5,1,0)
table(Predict = pred, tested$Direction)
##2
data(eba1977)
##2
install.packages("ISwR")
library(ISwR)
data(eba1977)
summary(eba1977)
View(eba1977)
fit1 <- glm(cases ~ city + age, offset = log(pop), data = eba1977, family = possion)
fit1 <- glm(cases ~ city + age, offset = log(pop), data = eba1977, family = poisson)
summary(fit1)
install.packages("qcc")
install.packages("qcc")
library(qcc)
qcc.overdispersion.test(CreditCard$active, type = "poisson")
qcc.overdispersion.test(eba1977$cases, type = "poisson")
library(AER)
dispersiontest(fit1)
pchisq(deviance(fit1), df.residual(fit1), lower.tail=FALSE)
##SRS
library(foreign)
kidiq <- read.dta(file="kidiq.dta")
##SRS
library(foreign)
kidiq <- read.dta(file="kidiq.dta")
kidiq <- read.dta(file="kidiq.dta")
kidiq <- read.csv("C:/Users/ingri/R learning/Lecture 6-20230324/kidiq.dta", sep="")
View(kidiq)
kidiq <- read.dta(file="kidiq.dta")
View(kidiq)
#ch8 exercise
fnc = function(d,i){
temp <- d[i]
return(mean(temp))
}
x <- c(1:50)
library(boot)
b = boot(x, fnc, 1000)
b$t
plot(b)
hist(b$t, breeak = 100)
hist(b$t, breeaks = 100)
warings()
hist(b$t, breaks = 100)
abline(v = 25.5, color = 'red')
abline(v = 25.5, col = 'red')
#ch8 exercise
fnc = function(d,i){
temp <- d[i]
return(mean(temp))
}
x <- c(1:50)
library(boot)
b = boot(x, fnc, 1000)
b$t
plot(b)
hist(b$t, breaks = 100)
abline(v = 25.5, col = 'red')
abline(v = mean(b$t), color = 'green')
abline(v = 25.5, col = 'red')
temp <- d[i]
x <- c(1:50)
library(boot)
b = boot(x, fnc, 1000)
b$t
plot(b)
hist(b$t, breaks = 100)
abline(v = 25.5, col = 'red')
abline(v = mean(b$t), col = 'green')
library(datasets)
data(iris)
library(datasets)
data(iris)
force(iris)
################
# UNSUPERVISED #
################
### Kmean ###
k = kmeans(iris[,1:4], centers=3) # k=3
k$size
library(useful)
#library(tidyverse)
plot(k,data=iris,class="Species")
## or
install.packages("factoextra")
library(factoextra)
fviz_cluster(k,           # 分群結果
data = iris[,1:4],              # 資料
geom = c("point","text"), # 點和標籤(point & label)
ellipse.type = "norm")      # 框架型態
# Elbow Method for K-Means (find the best k by SSE) 手肘方法 k unknown
fviz_nbclust(iris[,1:4],
FUNcluster = kmeans,# K-Means
method = "wss",     # total within sum of square
k.max = 20          # max number of clusters to consider
) +
labs(title="Elbow Method for K-Means")+
geom_vline(xintercept = 3, linetype = 2)
fviz_nbclust(iris[,-5], kmeans, method = "silhouette")
# Compute gap statistic for kmeans
# Recommended value for B is ~500
library(cluster)
gap_stat <- clusGap(iris[,-5], FUN = kmeans, nstart = 25,
K.max = 10, B = 500)
fviz_gap_stat(gap_stat)
### HC tree ###
E.dist <- dist(iris[,1:4], method="euclidean") # 歐式距離
M.dist <- dist(iris[,1:4], method="manhattan") # 曼哈頓距離
# visualize
fviz_dist(E.dist,gradient = list(low = "red", mid = "pink", high = "white")) #heatmap
# By Euclidean Distance
tree1 <- hclust(E.dist, method="ward.D2")
plot(tree1, xlab="Euclidean",h=-1)
abline(h=10,col="red")
data <- read.csv("Mall_Customers.csv",sep=",")
Mall_Customers <- read.csv("C:/Users/ingri/R learning/Lecture 9-20230414/Mall_Customers.csv")
View(Mall_Customers)
library(tidyverse)
#scale the data
newdata <- data %>%
mutate(
gender = ifelse(Gender=='Male',1,0),
#age = (Age - min(Age)) / (max(Age) - min(Age)),
annual.income = (AnnualIncome - min(AnnualIncome)) / (max(AnnualIncome) - min(AnnualIncome)),
spending.score = (SpendingScore - min(SpendingScore)) /(max(SpendingScore) - min(SpendingScore))
)
newdata <- data %>%
mutate(
gender = ifelse(Gender=='Male',1,0),
#age = (Age - min(Age)) / (max(Age) - min(Age)),
annual.income = (AnnualIncome - min(AnnualIncome)) / (max(AnnualIncome) - min(AnnualIncome)),
spending.score = (SpendingScore - min(SpendingScore)) /(max(SpendingScore) - min(SpendingScore))
)
data <- read.csv("Mall_Customers.csv",sep=",")
View(Mall_Customers)
setwd("C:/Users/ingri/R learning/Lecture 9-20230414")
data <- read.csv("Mall_Customers.csv",sep=",")
library(tidyverse)
#scale the data
newdata <- data %>%
mutate(
gender = ifelse(Gender=='Male',1,0),
#age = (Age - min(Age)) / (max(Age) - min(Age)),
annual.income = (AnnualIncome - min(AnnualIncome)) / (max(AnnualIncome) - min(AnnualIncome)),
spending.score = (SpendingScore - min(SpendingScore)) /(max(SpendingScore) - min(SpendingScore))
)
view(newdata)
newdata<-newdata[,-c(2,4,5)]
ggplot(newdata, aes(x=spending.score, y=annual.income)) +
geom_point()
#elbow method to find k
fviz_nbclust(newdata[,4:5],
FUNcluster = kmeans,# K-Means
method = "wss",     # total within sum of square
k.max = 20          # max number of clusters to consider
)
# Average silhouette for kmeans
fviz_nbclust(newdata[,4:5], FUN = kmeans, method = "silhouette")
fviz_nbclust(newdata[,4:5], FUN = kmeans, method = "wss")
km <- kmeans(newdata[,4:5], centers=5,nstart=20)
plot(km, data=newdata, class="gender")
table(km$cluster)
cc = km$cluster
library(factoextra)
fviz_cluster(km,           # 分群結果
data = newdata[,4:5],              # 資料
geom = c("point","text"), # 點和標籤(point & label)
ellipse.type = "norm")      # 框架型態
data = cbind(data,cc)
ggplot(data, aes(x=SpendingScore, y=AnnualIncome,color=as.factor(cc))) +
geom_point()
#hc method
tree1 <- dist(newdata[,1:4], method = "euclidean")
#hc method
E.dist <- dist(newdata[,1:4], method = "euclidean")
#hc method
E.dist <- dist(newdata[,1:4], method = "euclidean")
tree1 <- hclust(E.dist, method = "ward.D2")
View(tree1)
plot(tree1, xlab="Euclidean",h=-1)
abline(h=10,col="red")
plot(tree1, xlab="Euclidean",h=-1, cex = 0.6)
abline(h=10,col="red")
km <- kmeans(newdata[,4:5], centers=5,nstart=20)
plot(km, data=newdata, class="gender")
table(km$cluster)
cc = km$cluster
fviz_cluster(km,           # 分群結果
data = newdata[,4:5],              # 資料
geom = c("point","text"), # 點和標籤(point & label)
ellipse.type = "norm")      # 框架型態
data = cbind(data,cc)
# visualize
fviz_dist(E.dist,gradient = list(low = "red", mid = "pink", high = "white")) #heatmap
# Gap statistic for hierarchical clustering
fviz_nbclust(iris[,1:4], FUN = hcut, method = "wss")
fviz_nbclust(newdata[,1:4], FUN = hcut, method = "wss")
fviz_dist(E.dist,gradient = list(low = "red", mid = "pink", high = "white")) #heatmap
cluster
cluster <- cutree(tree1, k = 3)
cluster
table(cluster)
rect.hclust(tree1,k=3,border="red")
rect.hclust(tree1,k=3,border="red")
cluster <- cutree(tree1, k = 3)
E.dist <- dist(newdata[,1:4], method = "euclidean")
tree1 <- hclust(E.dist, method = "ward.D2")
plot(tree1, xlab="Euclidean",h=-1, cex = 0.6)
abline(h=10,col="red")
fviz_dist(E.dist,gradient = list(low = "red", mid = "pink", high = "white")) #heatmap
fviz_nbclust(newdata[,1:4], FUN = hcut, method = "wss")
cluster <- cutree(tree1, k = 3)
table(cluster)
rect.hclust(tree1,k=3,border="red")
which(cluster==1)
table(cluster)
which(cluster==1)
rect.hclust(tree1,k=3,border="red")
# find the best cluster size
p = fviz_nbclust(iris[,1:4],
FUNcluster = hcut,  # hierarchical clustering
method = "wss",     # total within sum of square
k.max = 20          # max number of clusters to consider
)
p
(p = p + labs(title="Elbow Method for HC") )
p + geom_vline(xintercept = 3,       # 在 X=3的地方
linetype = 2)
ac_ward <- agnes(E.dist, method = "ward")
ac_ward$ac
pltree(ac_ward, hang = -1) #, main="tree"
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")
# function to compute coefficient
ac <- function(x) {
agnes(E.dist, method = x)$ac
}
sapply(m,ac)
fviz_dist(E.dist,gradient = list(low = "red", mid = "pink", high = "white")) #heatmap
fviz_nbclust(newdata[,1:4], FUN = hcut, method = "wss")
#hc method
E.dist <- dist(newdata[,2:5], method = "euclidean")
tree1 <- hclust(E.dist, method = "ward.D2")
plot(tree1, xlab="Euclidean",h=-1, cex = 0.6)
abline(h=10,col="red")
fviz_dist(E.dist,gradient = list(low = "red", mid = "pink", high = "white")) #heatmap
fviz_nbclust(newdata[,1:4], FUN = hcut, method = "wss")
View(tree1)
View(Mall_Customers)
fviz_nbclust(newdata[,1:4], FUN = hcut, method = "wss")
cluster <- cutree(tree1, k = 3)
table(cluster)
rect.hclust(tree1,k=3,border="red")
data = cbind(data,cutree)
data = cbind(data,cluster)
ggplot(data, aes(x=SpendingScore, y=AnnualIncome,color=as.factor(cluster))) +
geom_point()
E.dist <- dist(newdata[,2:5], method = "euclidean")
newdata <- data %>%
mutate(
gender = ifelse(Gender=='Male',1,0),
#age = (Age - min(Age)) / (max(Age) - min(Age)),
annual.income = (AnnualIncome - min(AnnualIncome)) / (max(AnnualIncome) - min(AnnualIncome)),
spending.score = (SpendingScore - min(SpendingScore)) /(max(SpendingScore) - min(SpendingScore))
)
data <- read.csv("Mall_Customers.csv",sep=",")
library(tidyverse)
#scale the data
newdata <- data %>%
mutate(
gender = ifelse(Gender=='Male',1,0),
#age = (Age - min(Age)) / (max(Age) - min(Age)),
annual.income = (AnnualIncome - min(AnnualIncome)) / (max(AnnualIncome) - min(AnnualIncome)),
spending.score = (SpendingScore - min(SpendingScore)) /(max(SpendingScore) - min(SpendingScore))
)
view(newdata)
E.dist <- dist(newdata[,2:5], method = "euclidean")
tree1 <- hclust(E.dist, method = "ward.D2")
plot(tree1, xlab="Euclidean",h=-1, cex = 0.6)
abline(h=10,col="red")
cluster <- cutree(tree1, k = 3)
table(cluster)
data = cbind(data,cluster)
ggplot(data, aes(x=SpendingScore, y=AnnualIncome,color=as.factor(cluster))) +
geom_point()
ggplot(data, aes(x=as.factor(cluster), y=Age)) +
geom_boxplot()
ggplot(data, aes(x=as.factor(cluster), y=AnnualIncome)) +
geom_boxplot()
ggplot(data, aes(x=as.factor(cluster), y=SpendingScore)) +
geom_boxplot()
ggplot( data =data) +
geom_bar( aes( x = Gender)) +
facet_wrap( ~ cluster)
cluster <- cutree(tree1, k = 4)
table(cluster)
data = cbind(data,cluster)
ggplot(data, aes(x=SpendingScore, y=AnnualIncome,color=as.factor(cluster))) +
geom_point()
data = cbind(data,cluster)
ggplot(data, aes(x=SpendingScore, y=AnnualIncome,color=as.factor(cluster))) +
geom_point()
ggplot(data, aes(x=as.factor(cluster), y=Age)) +
geom_boxplot()
data <- read.csv("Mall_Customers.csv",sep=",")
library(tidyverse)
#scale the data
newdata <- data %>%
mutate(
gender = ifelse(Gender=='Male',1,0),
#age = (Age - min(Age)) / (max(Age) - min(Age)),
annual.income = (AnnualIncome - min(AnnualIncome)) / (max(AnnualIncome) - min(AnnualIncome)),
spending.score = (SpendingScore - min(SpendingScore)) /(max(SpendingScore) - min(SpendingScore))
)
E.dist <- dist(newdata[,2:5], method = "euclidean")
tree1 <- hclust(E.dist, method = "ward.D2")
plot(tree1, xlab="Euclidean",h=-1, cex = 0.6)
abline(h=10,col="red")
fviz_dist(E.dist,gradient = list(low = "red", mid = "pink", high = "white")) #heatmap
fviz_nbclust(newdata[,1:4], FUN = hcut, method = "wss")
cluster <- cutree(tree1, k = 4)
table(cluster)
data = cbind(data,cluster)
ggplot(data, aes(x=SpendingScore, y=AnnualIncome,color=as.factor(cluster))) +
geom_point()
data <- read.csv("Mall_Customers.csv",sep=",")
library(tidyverse)
#scale the data
newdata <- data %>%
mutate(
gender = ifelse(Gender=='Male',1,0),
#age = (Age - min(Age)) / (max(Age) - min(Age)),
annual.income = (AnnualIncome - min(AnnualIncome)) / (max(AnnualIncome) - min(AnnualIncome)),
spending.score = (SpendingScore - min(SpendingScore)) /(max(SpendingScore) - min(SpendingScore))
)
E.dist <- dist(newdata[,2:5], method = "euclidean")
tree1 <- hclust(E.dist, method = "ward.D2")
plot(tree1, xlab="Euclidean",h=-1, cex = 0.6)
abline(h=10,col="red")
fviz_dist(E.dist,gradient = list(low = "red", mid = "pink", high = "white")) #heatmap
fviz_nbclust(newdata[,1:4], FUN = hcut, method = "wss")
cluster <- cutree(tree1, k = 5)
table(cluster)
data = cbind(data,cluster)
ggplot(data, aes(x=SpendingScore, y=AnnualIncome,color=as.factor(cluster))) +
geom_point()
ggplot(data, aes(x=as.factor(cluster), y=Age)) +
geom_boxplot()
ggplot(data, aes(x=as.factor(cluster), y=AnnualIncome)) +
geom_boxplot()
ggplot(data, aes(x=as.factor(cluster), y=SpendingScore)) +
geom_boxplot()
ggplot( data =data) +
geom_bar( aes( x = Gender)) +
facet_wrap( ~ cluster)
fviz_nbclust(newdata[,2:5], FUN = hcut, method = "wss")
data = cbind(data,cluster)
ggplot(data, aes(x=SpendingScore, y=AnnualIncome,color=as.factor(cluster))) +
geom_point()
data <- read.csv("Mall_Customers.csv",sep=",")
library(tidyverse)
#scale the data
newdata <- data %>%
mutate(
gender = ifelse(Gender=='Male',1,0),
#age = (Age - min(Age)) / (max(Age) - min(Age)),
annual.income = (AnnualIncome - min(AnnualIncome)) / (max(AnnualIncome) - min(AnnualIncome)),
spending.score = (SpendingScore - min(SpendingScore)) /(max(SpendingScore) - min(SpendingScore))
)
E.dist <- dist(newdata[,2:5], method = "euclidean")
tree1 <- hclust(E.dist, method = "ward.D2")
plot(tree1, xlab="Euclidean",h=-1, cex = 0.6)
abline(h=10,col="red")
fviz_dist(E.dist,gradient = list(low = "red", mid = "pink", high = "white")) #heatmap
fviz_nbclust(newdata[,2:5], FUN = hcut, method = "wss")
cluster <- cutree(tree1, k = 5)
table(cluster)
data = cbind(data,cluster)
ggplot(data, aes(x=SpendingScore, y=AnnualIncome,color=as.factor(cluster))) +
geom_point()
ggplot(data, aes(x=as.factor(cluster), y=Age)) +
geom_boxplot()
ggplot(data, aes(x=as.factor(cluster), y=AnnualIncome)) +
geom_boxplot()
ggplot(data, aes(x=as.factor(cluster), y=SpendingScore)) +
geom_boxplot()
ggplot( data =data) +
geom_bar( aes( x = Gender)) +
facet_wrap( ~ cluster)
